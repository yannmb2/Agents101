#!/usr/bin/env python
"""
rag_demo_mistral.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Minimal Retrieval-Augmented-Generation demo avec Mistral AI:

  â€¢ RecursiveCharacterTextSplitter (LangChain utility only)
  â€¢ PDF + TXT ingestion
  â€¢ FAISS vector search
  â€¢ Mistral embeddings + chat
  â€¢ Chat history inside the loop

Dependencies
------------
pip install mistralai==1.* faiss-cpu numpy pypdf langchain tqdm python-dotenv
"""

from __future__ import annotations
import os, json, textwrap
from pathlib import Path
from typing import List, Dict, Any

import faiss, numpy as np
from mistralai import Mistral
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pypdf import PdfReader               # <â”€ PDF extractor
from tqdm.auto import tqdm                # optional progress bar
from dotenv import load_dotenv

load_dotenv()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DOCS_DIR        = Path("./Data")             # put .txt & .pdf files here
EMBED_MODEL     = "mistral-embed"
CHAT_MODEL      = "mistral-large-latest"     # ou "mistral-medium-latest"
CHUNK_SIZE      = 800                        # characters
CHUNK_OVERLAP   = 150
TOP_K           = 4

SYSTEM_PROMPT = (
    "Tu es un tuteur prÃ©cis et concis. "
    "RÃ©ponds UNIQUEMENT Ã  partir du contexte fourni. "
    "Si la rÃ©ponse est manquante, dis \"Je ne sais pas.\""
)

SYSTEM_PROMPT_CHECK = (
    "Is the following answer well-supported by the provided context? Answer yes or no, and justify briefly in french"
)



api_key = os.getenv("MISTRAL_API_KEY")
assert api_key, "ğŸ‘‰  Veuillez dÃ©finir MISTRAL_API_KEY dans votre fichier .env!"

client = Mistral(api_key=api_key)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# 1ï¸âƒ£  Load & split ----------------------------------------------------------
def read_pdf_text(path: Path) -> str:
    """Extrait le texte d'un fichier PDF."""
    reader = PdfReader(str(path))
    return "\n".join(page.extract_text() or "" for page in reader.pages)

def load_and_split() -> List[str]:
    """Charge et divise les documents en chunks."""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE, 
        chunk_overlap=CHUNK_OVERLAP
    )
    chunks: List[str] = []
    
    print(f"ğŸ”  Recherche de fichiers dans {DOCS_DIR}")
    
    for path in DOCS_DIR.rglob("*"):
        if path.suffix.lower() == ".txt":
            print(f"ğŸ“„  Traitement: {path.name}")
            text = path.read_text(encoding="utf-8", errors="ignore")
        elif path.suffix.lower() == ".pdf":
            print(f"ğŸ“‘  Traitement: {path.name}")
            text = read_pdf_text(path)
        else:
            continue
        
        file_chunks = splitter.split_text(text)
        # Ajouter le nom du fichier Ã  chaque chunk pour le contexte
        for chunk in file_chunks:
            chunks.append(f"[Source: {path.name}]\n{chunk}")
    
    if not chunks:
        raise RuntimeError(f"Aucun fichier .txt ou .pdf trouvÃ© dans {DOCS_DIR}")
    
    print(f"âœ…  {len(chunks)} chunks crÃ©Ã©s")
    return chunks

# 2ï¸âƒ£  Mistral embeddings -----------------------------------------------------
def embed(texts: List[str]) -> List[List[float]]:
    """GÃ©nÃ¨re les embeddings avec Mistral."""
    try:
        # Mistral peut traiter plusieurs textes Ã  la fois
        response = client.embeddings.create(
            model=EMBED_MODEL,
            inputs=texts
        )
        return [data.embedding for data in response.data]
    except Exception as e:
        print(f"âŒ  Erreur lors de la gÃ©nÃ©ration des embeddings: {e}")
        raise

# 3ï¸âƒ£  Build (or load) FAISS --------------------------------------------------
def get_faiss_store(chunks: List[str], idx_path: str = "faiss_mistral.index") -> tuple:
    """Construit ou charge l'index FAISS."""
    meta_path = idx_path + ".meta.json"
    
    if Path(idx_path).exists() and Path(meta_path).exists():
        print("âœ“  Chargement de l'index vectoriel existant â€¦")
        index = faiss.read_index(idx_path)
        stored_chunks = json.loads(Path(meta_path).read_text(encoding='utf-8'))
        return index, stored_chunks

    print("â³  Construction de l'index vectoriel â€¦")
    all_vectors = []
    
    # Traitement par batch pour Ã©viter les limites de l'API
    batch_size = 32  # Mistral recommande des batches plus petits
    for i in tqdm(range(0, len(chunks), batch_size), desc="GÃ©nÃ©ration embeddings"):
        batch = chunks[i:i + batch_size]
        batch_vectors = embed(batch)
        all_vectors.extend(batch_vectors)
    
    # Conversion en matrice numpy
    mat = np.asarray(all_vectors, dtype=np.float32)
    
    # CrÃ©ation et sauvegarde de l'index FAISS
    index = faiss.IndexFlatL2(mat.shape[1])
    index.add(mat)
    faiss.write_index(index, idx_path)
    
    # Sauvegarde des mÃ©tadonnÃ©es
    Path(meta_path).write_text(json.dumps(chunks, ensure_ascii=False), encoding='utf-8')
    
    print(f"âœ…  Index sauvegardÃ© avec {len(chunks)} documents")
    return index, chunks

# 4ï¸âƒ£  Retrieval --------------------------------------------------------------
def retrieve(query: str, index, chunks: List[str], k: int = TOP_K) -> List[str]:
    """RÃ©cupÃ¨re les chunks les plus pertinents."""
    q_vec = np.asarray(embed([query])[0], dtype=np.float32).reshape(1, -1)
    distances, idxs = index.search(q_vec, k)
    
    retrieved_chunks = [chunks[i] for i in idxs[0]]
    
    # Affichage des scores de similaritÃ© pour le debug
    print(f"ğŸ“Š  Scores de similaritÃ©: {distances[0]}")
    
    return retrieved_chunks

# 5ï¸âƒ£  Prompt helpers ---------------------------------------------------------
def build_user_prompt(question: str, ctx_chunks: List[str]) -> str:
    """Construit le prompt utilisateur avec le contexte."""
    context_block = "\n\n".join(
        f"[Document {i+1}]\n{chunk}" for i, chunk in enumerate(ctx_chunks)
    )
    return (
        "Utilise le contexte ci-dessous pour rÃ©pondre Ã  la question.\n\n"
        f"Contexte:\n{context_block}\n\n"
        f"Question: {question}\nRÃ©ponse:"
    )

# 6ï¸âƒ£  Chat loop with history -------------------------------------------------
def chat_loop(index, chunks: List[str]):
    """Boucle de chat interactive avec historique."""
    history: List[Dict[str, str]] = []  # stocke les tours prÃ©cÃ©dents
    
    print("\nğŸ¯  RAG avec Mistral AI activÃ©!")
    print("   Tapez votre question ou Ctrl-C pour quitter")
    
    while True:
        try:
            q = input("\nğŸ’¬  Question: ").strip()
            if not q:
                continue
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹  Au revoir!")
            break

        # RÃ©cupÃ©ration du contexte pertinent
        ctx = retrieve(q, index, chunks)
        user_prompt = build_user_prompt(q, ctx)

        # Construction des messages pour l'API
        messages = [{"role": "system", "content": SYSTEM_PROMPT}]
        messages.extend(history)
        messages.append({"role": "user", "content": user_prompt})

        # Affichage du contexte rÃ©cupÃ©rÃ© (transparence)
        print("\nğŸ”  Contexte rÃ©cupÃ©rÃ©:")
        print("â”€" * 80)
        for i, c in enumerate(ctx, 1):
            # Troncature pour l'affichage
            display_chunk = c[:200] + "..." if len(c) > 200 else c
            print(f"[Doc {i}] {display_chunk}")
        print("â”€" * 80)

        try:
            # Appel Ã  l'API Mistral
            response = client.chat.complete(
                model=CHAT_MODEL,
                messages=messages,
                temperature=0.2,
                max_tokens=1000
            )
            
            answer = response.choices[0].message.content
            
            print("ğŸ¤–  RÃ©ponse:\n")
            print(textwrap.fill(answer, width=88))
            
            # Mise Ã  jour de l'historique avec la question simple et la rÃ©ponse
            history.extend([
                {"role": "user", "content": q},
                {"role": "assistant", "content": answer},
            ])
            
            # Limitation de l'historique pour Ã©viter des contextes trop longs
            if len(history) > 10:  # Garde les 5 derniers Ã©changes
                history = history[-10:]
                
        except Exception as e:
            print(f"âŒ  Erreur lors de l'appel Ã  Mistral: {e}")
            continue

# 6ï¸âƒ£  Chat loop with history -------------------------------------------------
def chat_loop_with_check(index, chunks: List[str]):
    """Boucle de chat interactive avec nÅ“ud de vÃ©rification."""
    history: List[Dict[str, str]] = []  # stocke les tours prÃ©cÃ©dents
    
    print("\nğŸ¯  RAG avec Mistral AI activÃ© (avec nÅ“ud de vÃ©rification)!")
    print("   Tapez votre question ou Ctrl-C pour quitter")
    
    while True:
        try:
            q = input("\nğŸ’¬  Question: ").strip()
            if not q:
                continue
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹  Au revoir!")
            break

        # RÃ©cupÃ©ration du contexte pertinent
        ctx = retrieve(q, index, chunks)
        user_prompt = build_user_prompt(q, ctx)

        # Construction des messages pour l'API
        messages = [{"role": "system", "content": SYSTEM_PROMPT}]
        messages.extend(history)
        messages.append({"role": "user", "content": user_prompt})

        # Affichage du contexte rÃ©cupÃ©rÃ© (transparence)
        print("\nğŸ”  Contexte rÃ©cupÃ©rÃ©:")
        print("â”€" * 80)
        for i, c in enumerate(ctx, 1):
            # Troncature pour l'affichage
            display_chunk = c[:200] + "..." if len(c) > 200 else c
            print(f"[Doc {i}] {display_chunk}")
        print("â”€" * 80)

        try:
            # Appel Ã  l'API Mistral pour la rÃ©ponse principale
            response = client.chat.complete(
                model=CHAT_MODEL,
                messages=messages,
                temperature=0.2,
                max_tokens=1000
            )
            
            answer = response.choices[0].message.content
            
            # === NÅ’UD DE VÃ‰RIFICATION ===
            context_for_check = "\n\n".join(ctx)
            verification_prompt = (
                f"Context:\n{context_for_check}\n\n"
                f"Answer to verify:\n{answer}\n\n"
                f"Is this answer well-supported by the provided context?"
            )
            
            # Appel au nÅ“ud de vÃ©rification
            response_check = client.chat.complete(
                model=CHAT_MODEL,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT_CHECK},
                    {"role": "user", "content": verification_prompt}
                ],
                temperature=0.1,  # Plus strict pour la vÃ©rification
                max_tokens=200
            )
            
            verification_result = response_check.choices[0].message.content.lower()
            
            # Analyse du rÃ©sultat de vÃ©rification
            is_reliable = "yes" in verification_result or "oui" in verification_result
            
            # Affichage avec flag de fiabilitÃ©
            if is_reliable:
                print("ğŸ¤–  RÃ©ponse (âœ… VÃ©rifiÃ©e):\n")
                print(textwrap.fill(answer, width=88))
            else:
                print("ğŸ¤–  RÃ©ponse (âš ï¸  NON FIABLE - Contexte insuffisant):\n")
                print(textwrap.fill(answer, width=88))
                print("\nâŒ  ATTENTION: Cette rÃ©ponse pourrait ne pas Ãªtre bien supportÃ©e par les documents fournis.")
            
            print(f"\nğŸ”  Analyse de vÃ©rification: {response_check.choices[0].message.content}")
            print()
            
            # Mise Ã  jour de l'historique avec la question simple et la rÃ©ponse
            history.extend([
                {"role": "user", "content": q},
                {"role": "assistant", "content": answer},
            ])
            
            # Limitation de l'historique pour Ã©viter des contextes trop longs
            if len(history) > 10:  # Garde les 5 derniers Ã©changes
                history = history[-10:]
                
        except Exception as e:
            print(f"âŒ  Erreur lors de l'appel Ã  Mistral: {e}")
            continue

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    """Fonction principale."""
    try:
        print("ğŸš€  DÃ©marrage du RAG avec Mistral AI")
        
        # VÃ©rification du dossier de donnÃ©es
        if not DOCS_DIR.exists():
            print(f"ğŸ“  CrÃ©ation du dossier {DOCS_DIR}")
            DOCS_DIR.mkdir(parents=True, exist_ok=True)
            print(f"ğŸ‘‰  Placez vos fichiers .txt et .pdf dans {DOCS_DIR}")
            return
        
        # Chargement et traitement des documents
        chunks = load_and_split()
        
        # Construction/chargement de l'index
        index, chunks = get_faiss_store(chunks)
        
        # Lancement de la boucle de chat
        chat_loop(index, chunks)
        
    except Exception as e:
        print(f"ğŸ’¥  Erreur: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())